{"nbformat":4,"nbformat_minor":0,"cells":[{"cell_type":"markdown","source":["# 3. ðŸŽ‹ Decision Trees/Random Forest:\nExported from Filament on Thu, 17 Mar 2022 19:29:26 GMT\n\n---"],"metadata":{}},{"cell_type":"markdown","source":[""],"metadata":{}},{"cell_type":"code","source":["import numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom time import time\n\nfrom sklearn import datasets\nfrom sklearn import metrics\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["df = pd.read_csv('cleaned_mushrooms.csv')\n\n# reading in the cleaned data"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"markdown","source":["### âœ…  Functions:"],"metadata":{}},{"cell_type":"code","source":["# Our Accuracy, precision, recall and f1 scores custom function\n\ndef apr(y_pred, y_real):\n    accuracy = metrics.accuracy_score(y_real, y_pred)\n    precision = metrics.precision_score(y_real, y_pred)\n    recall = metrics.recall_score(y_real, y_pred)\n    f1 = metrics.f1_score(y_real, y_pred)\n    \n    print(f\"Accuracy:{accuracy}\")\n    print(f\"Precision:{precision}\")\n    print(f\"Recall:{recall}\")\n    print(f\"F1:{f1}\")\n    return accuracy, precision, recall, f1\n\n\n# Confusion matrix function\n\ndef produce_confusion(positive_label, negative_label, cut_off, df, y_pred_name, y_real_name):\n    # Set pred to 0 or 1 depending on whether it's higher than the cut_off point.\n    # We use this when we predict probabilites\n    if cut_off != 'binary':      \n        df['pred_binary'] = np.where(df[y_pred_name] > cut_off , 1, 0)\n    else: \n        df['pred_binary'] = df[y_pred_name]\n    \n    # Build the matrix\n    cm = confusion_matrix(df[y_real_name], df['pred_binary'])  \n    \n    ax= plt.subplot()\n    sns.heatmap(cm, annot=True, ax=ax, fmt='g'); \n\n    # labels\n    ax.set_xlabel('Predicted labels');\n    ax.set_ylabel('Real labels'); \n    # Title\n    ax.set_title('Confusion Matrix'); \n    # Ticks\n    ax.xaxis.set_ticklabels([negative_label, positive_label])\n    ax.yaxis.set_ticklabels([negative_label, positive_label]);\n\n    print('Test accuracy = ', accuracy_score(df[y_real_name], df['pred_binary']))\n\n    return accuracy_score(df[y_real_name], df['pred_binary'])"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"markdown","source":["## âœ…  Label Encoding:"],"metadata":{}},{"cell_type":"code","source":["# columns with only two categories can be label encoded:\n\nfor col in df.columns:\n    if len(df[col].value_counts()) == 2:\n        df[col] = df[col].astype('category') ## change these cols to category data types\n        df[col] = df[col].cat.codes ## use cat.codes function to label encode\n\ndf.dtypes # we can see the data types of the label encoded cols are now integers"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["# prints every column and its unique values\n\nfor col in df.columns:\n    print(col, \" : \", df[col].unique())\n"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["# we know edible = 4208, and poisionous = 3916\n# double checking that 0 = edible, 1 = poisonous\n\ndf['class'].value_counts()"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"markdown","source":["## âœ… OHE:"],"metadata":{}},{"cell_type":"code","source":["# one hot encoding all columns that haven't been label encoded\n\none_hot = list(df.columns)\none_hot.remove('class')\none_hot.remove('bruises')\none_hot.remove('gill_attachment')\none_hot.remove('gill_spacing')\none_hot.remove('gill_size')\none_hot.remove('stalk_shape')\ndf = pd.get_dummies(df, columns = one_hot, prefix = one_hot)"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["# assigning variables for ease later on \n\nPoisonous = df[df['class'] == 1]\nEdible = df[df['class'] == 0]\nfeature_cols = list(df.columns)\nfeature_cols.remove('class')"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["# saved here so that we can reuse the ohe and label encoded version to run more refined decision tree models later\n\ndf.to_csv('decision_mushrooms.csv', index = False)"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"markdown","source":["## âœ… Train/Test Split:"],"metadata":{}},{"cell_type":"code","source":["# train size = 80%, test size = 20%, random state = 124\n\nX = df.drop(columns = ['class'])\ny = df['class']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=124, stratify=y)"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"markdown","source":["# ðŸŽ‹ Decision Trees:"],"metadata":{}},{"cell_type":"code","source":["# here we randomly set the max depth to 3 and fit the decision tree\n\ntreeclf = DecisionTreeClassifier(max_depth=3, random_state=1)\ntreeclf.fit(X, y)"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["# here we call the decision tree- we can see the results are ok, but certainly room for improvement!\n\nplt.figure(figsize=(30,20))\ntree.plot_tree(treeclf, feature_names=feature_cols,  \n               class_names=['Poisonous','Edible'],filled = True)"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["# printing the results for the train and test data\n# pretty good results, especially considering we inputted random parameters\n\nprint(f'Score on training set: {treeclf.score(X_train, y_train)}')\nprint(f'Score on testing set: {treeclf.score(X_test, y_test)}')"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["# GRIDSEARCH:\n# we can fine tune our own hyperparameters to find the best cross validation score\n\ngrid = GridSearchCV(estimator = DecisionTreeClassifier(),\n                    param_grid = {'max_depth': [3, 5, 7, 10, 15],\n                                  'min_samples_split': [5, 10, 15, 20],\n                                  'min_samples_leaf': [2, 3, 4, 5, 6, 7]},\n                    cv = 5,\n                    refit = True,\n                    verbose = 1,\n                    scoring = 'accuracy')"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["# this nifty code tells us how long the model takes to run\n\nnow = time()\n\ngrid.fit(X_train, y_train)\n\nprint(f' Time in seconds: {time() - now}')"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["# this tells us the best parameters from our gridsearch\n\ngrid.best_params_"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["# Gives us the best average of all c-v folds for a single combination of the params we've inputted\n\ngrid.best_score_"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["# lets rerun the decision trees using these new params\n# we've saved it as a new variable (dt)\n\ndt = DecisionTreeClassifier(max_depth=7, min_samples_leaf=2, min_samples_split=5)"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["dt.fit(X_train, y_train)"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["# nice- the results seem to be better- but lets check the scores to confirm\n\nplt.figure(figsize=(30,20))\ntree.plot_tree(dt, feature_names=feature_cols, \n               class_names=['Poisonous','Edible'],filled=True)"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["# wow the scores have improved- almost too good... overfitting?\n\nprint(f'Score on training set: {dt.score(X_train, y_train)}')\nprint(f'Score on testing set: {dt.score(X_test, y_test)}')"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["# here we make a copy of the train data\n# then we add actual and prediction columns, followed by the probability of ascertaining the right result\n\ntrain_results = X_train.copy()\ntrain_results['y_pred'] = dt.predict(X_train)\ntrain_results['y_real'] = y_train\ntrain_results['y_prob'] = dt.predict_proba(X_train)[:,1]\n\ntrain_results"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["# we repeat the process for the test data\n\ntest_results = X_test.copy()\ntest_results['y_pred'] = dt.predict(X_test)\ntest_results['y_real'] = y_test\ntest_results['y_prob'] = dt.predict_proba(X_test)[:,1]\n\ntest_results"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"markdown","source":["## âœ… Feature Importance:"],"metadata":{}},{"cell_type":"code","source":["# looking at the feature importance of our decision tree\n\ndt.feature_importances_"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["# turn the feature importance into a list (need to zip first) to make it easy tor read\n# the higher the score the better!\n\nimportance = list(zip(feature_cols, list(dt.feature_importances_)))\nimportance"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"markdown","source":[""],"metadata":{}},{"cell_type":"markdown","source":["These are all the features with importance above zero:"],"metadata":{}},{"cell_type":"markdown","source":["gill_size, cap_surface_grooves, odor_almond, odor_anise, odor_none, stalk_root_bulbous, stalk_root_club, stalk_surface_below_ring_scaly, spore_print_color_green, population_clustered, habitat_woods"],"metadata":{}},{"cell_type":"markdown","source":[""],"metadata":{}},{"cell_type":"markdown","source":["We can use these in a refined decision trees model, and see what results we get- see 'Dec Trees- Final' workbook!"],"metadata":{}},{"cell_type":"markdown","source":["But lets carry on below for now..."],"metadata":{}},{"cell_type":"code","source":["# we've brought in our confusion matrix to see our train results\n\n\"\"\" Confusion Matrix for Train Data \"\"\"\nproduce_confusion('Poisonous', 'Edible', 0.5, train_results, 'y_pred', 'y_real')"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["# we do the same for the test data\n\nproduce_confusion('Poisonous','Edible', 0.5, test_results, 'y_pred', 'y_real')"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"markdown","source":[""],"metadata":{}},{"cell_type":"markdown","source":["# ðŸŽ‹ðŸŽ‹ Random Forest:"],"metadata":{}},{"cell_type":"markdown","source":["Random forests are another great way to model, lets try it below to see what results we get! Although the decision tree will be pretty hard to beat with results like that!"],"metadata":{}},{"cell_type":"code","source":["# now lets try random forest modelling, once again lets start with inputting a random parameter\n\nrf = RandomForestClassifier(n_estimators=50)"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["# here we have our accuracy, precision, recall and f1 scores for our random forest\n\nrf_score = cross_val_score(rf, X_train, y_train, cv=5)\nprint(f'Random scored {rf_score}')"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["#Although ensembling and random forests typically improve model results from decision trees\n#They still need to be subject to the usual tunning of hyperparameters to improve the model further\n#Gridsearch can be used for this\n\nrf_params = {\n    'n_estimators': list(range(1, 100, 2)),\n    'max_depth': [None, 1, 2, 3, 4, 5],\n}\n\n\ngs = GridSearchCV(rf, param_grid=rf_params, cv=5)\n\ngs.fit(X_train, y_train)\n\nprint(f'Best score: {gs.best_score_}')\n\ngs.best_params_"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["# once again we seem to have a perfect score!\n\ngs.score(X_train, y_train)"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["## Lets see our metrics score, comparing our predictions to the actual results\n## what a surprise- we have perfect scores across the board!\n\npredictions_rf_train = pd.DataFrame(index=X_train.index)\n\npredictions_rf_train['Pred'] = gs.predict(X_train)\npredictions_rf_train['Actual'] = y_train\n\napr(predictions_rf_train['Pred'],predictions_rf_train['Actual'])"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["\n## Check scores for the test data now- and once again we have perfect scores!\n\n\npredictions_rf_test = pd.DataFrame(index=X_test.index)\n\npredictions_rf_test['Pred'] = gs.predict(X_test)\npredictions_rf_test['Actual'] = y_test\n\napr(predictions_rf_test['Pred'],predictions_rf_test['Actual'])"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["# Here we see the most improtant features in our random forest\n# they are exactly the same as the 12 most important features or decision tree picked up\n# except here we've used a lovely visual\n\nfeat_imp = pd.DataFrame(dt.feature_importances_, index=X_train.columns, columns=[\"feat_imp\"])\nfeat_imp = feat_imp.sort_values(\"feat_imp\", ascending=False)\nfeat_imp.style.background_gradient(\"Blues\")\n"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"markdown","source":[""],"metadata":{}},{"cell_type":"markdown","source":[""],"metadata":{}},{"cell_type":"markdown","source":[""],"metadata":{}},{"cell_type":"markdown","source":[""],"metadata":{}}],"metadata":{"createdWith":"Filament"}}