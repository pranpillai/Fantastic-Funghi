{"nbformat":4,"nbformat_minor":0,"cells":[{"cell_type":"markdown","source":["# 6. ðŸ“ˆ KNN- Final\nExported from Filament on Thu, 17 Mar 2022 19:30:45 GMT\n\n---"],"metadata":{}},{"cell_type":"code","source":["import numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n##\nfrom sklearn import datasets\nimport sklearn.metrics as sm\nfrom sklearn.cluster import KMeans\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import classification_report, confusion_matrix"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["df = pd.read_csv('knn_mushrooms.csv') # reading in the cleaned and encoded mushroom data"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"markdown","source":["These are all the features with importance above zero:"],"metadata":{}},{"cell_type":"markdown","source":["gill_size, cap_surface_grooves, odor_almond, odor_anise, odor_none, stalk_root_bulbous, stalk_root_club, stalk_surface_below_ring_scaly, spore_print_color_green, population_clustered, habitat_woods"],"metadata":{}},{"cell_type":"markdown","source":["So lets just use these for our refined KNN model and see what results we get."],"metadata":{}},{"cell_type":"markdown","source":["## âœ… KNN Final"],"metadata":{}},{"cell_type":"code","source":["# train/test split using the refined list of features\n\nfeature_cols = ['gill_size', 'cap_surface_grooves', 'odor_almond', 'odor_anise', 'odor_none',\n    'stalk_root_bulbous', 'stalk_root_club', 'stalk_surface_below_ring_scaly',\n    'spore_print_color_green', 'population_clustered', 'habitat_woods']\ny = df['class']\nX = df[feature_cols]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=124, stratify=y)"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["# remembering to scale our data again\n\nscaler = MinMaxScaler()\n\nscaler.fit(X_train)\n\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["# creating that great visual to display our mean error rate\n\nerrors = []\n\nfor k in range(1, 40):\n    knn = KNeighborsClassifier(n_neighbors= k)\n    knn.fit(X_train, y_train)\n    pred_i = knn.predict(X_test)\n    errors.append(np.mean(pred_i != y_test))\n    \nplt.figure(figsize=(12, 6))\nplt.plot(range(1, 40)\n         , errors\n         , color='black'\n         , linestyle='dashed'\n         , marker='o'\n         , markerfacecolor='grey'\n         , markersize=10)\nplt.title('Error Rate K Value')\nplt.xlabel('K Value')\nplt.ylabel('Mean Error')\nplt.show()"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["# now we can see the best neighbour parameter is 4, so lets try this and go from there\n\nclassifier = KNeighborsClassifier(n_neighbors= 4)\nclassifier.fit(X_train, y_train)"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["# so here we have it- creating the preicition, and running the matrix and report\n\ny_pred = classifier.predict(X_test)\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"markdown","source":["## âœ… Conclusion:"],"metadata":{}},{"cell_type":"markdown","source":[""],"metadata":{}},{"cell_type":"markdown","source":["We can see comparatively that we have one more false negative in this refined KNN model, than we did in the original KNN model that used all the features. More importantly we can see that once again there are 2 false negatives! So unfortunately, the KNN method didn't improve our model either. In that case, lets tick with the final decision trees model, its far easier to interpret and we can see the full breakdown from node to node of how our data is being classified. "],"metadata":{}},{"cell_type":"markdown","source":["So if you ever find yourself lost and foraging for mushrooms- just remember to differentiate the edible from the poisonous using these features: it's gill size, cap surface grooves, an almond, anise or odourless mushroom, whether its got a bulbous or clubbed stalk root, if it has a scaly ring below the surface, if the spore is green, if its amongst a clustered population and if you're in the woods! Good luck- and remember there's still a slim chance you could get it wrong, so at the very least- at least make sure it looks like its worth it!"],"metadata":{}}],"metadata":{"createdWith":"Filament"}}