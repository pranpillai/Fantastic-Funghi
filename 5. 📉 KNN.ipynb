{"nbformat":4,"nbformat_minor":0,"cells":[{"cell_type":"markdown","source":["# 5. ðŸ“‰ KNN:\nExported from Filament on Thu, 17 Mar 2022 19:30:23 GMT\n\n---"],"metadata":{}},{"cell_type":"markdown","source":["I also experimented with KNN Clustering, just to see if I could improve on the 2 false negatives I got from my final Decision Tree. First lets run the model on all the features and see what we get, before refining down"],"metadata":{}},{"cell_type":"code","source":["import numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n##\nfrom sklearn import datasets\nimport sklearn.metrics as sm\nfrom sklearn.cluster import KMeans\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import classification_report, confusion_matrix"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["df = pd.read_csv('cleaned_mushrooms.csv') # reading in the cleaned mushroom data"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"markdown","source":["## âœ… Label Encoding:"],"metadata":{}},{"cell_type":"code","source":["# cols with only two categories can be label encoded:\n\nfor col in df.columns:\n    if len(df[col].value_counts()) == 2:\n        df[col] = df[col].astype('category') ## change these cols to category data types\n        df[col] = df[col].cat.codes ## use cat.codes function to label encode\n\ndf.dtypes"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["for col in df.columns:\n    print(col, \" : \", df[col].unique()) # just displays all the cols and their unique values"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"markdown","source":["## âœ… OHE:"],"metadata":{}},{"cell_type":"code","source":["# here we one hot encode all the cols that haven't already been label encoded\n\none_hot = list(df.columns)\none_hot.remove('class')\none_hot.remove('bruises')\none_hot.remove('gill_attachment')\none_hot.remove('gill_spacing')\none_hot.remove('gill_size')\none_hot.remove('stalk_shape')\ndf = pd.get_dummies(df, columns = one_hot, prefix = one_hot)"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["df.shape"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["# thought it would be useful to save here in case I refine my KNN model features\n\ndf.to_csv('knn_mushrooms.csv', index = False)"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"markdown","source":["## âœ… Train/Test Split:"],"metadata":{}},{"cell_type":"code","source":["# here we train/test split with tes size being 20%, and set a random state of 124\n# we also stratify to ensure the best representation is being provided\nX = df.drop(columns=['class'])\ny = df['class']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=124, stratify=y)"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["# we scale the data to make it easier for our model to interpret\n\nscaler = MinMaxScaler()\n\nscaler.fit(X_train)\n\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["# here we're displaying the error rate in a random range, in order to idnetify what the best n-neighbors parameter is\nerrors = []\n\nfor k in range(1, 100):\n    knn = KNeighborsClassifier(n_neighbors= k)\n    knn.fit(X_train, y_train)\n    pred_i = knn.predict(X_test)\n    errors.append(np.mean(pred_i != y_test))\n    \nplt.figure(figsize=(12, 6))\nplt.plot(range(1, 100)\n         , errors\n         , color='black'\n         , linestyle='dashed'\n         , marker='o'\n         , markerfacecolor='grey'\n         , markersize=10)\nplt.title('Error Rate K Value')\nplt.xlabel('K Value')\nplt.ylabel('Mean Error')\nplt.show()"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["# we can see from the plot above that 10 is the lowest mean error rate aove zero- great\n# lets run the model\n\nclassifier = KNeighborsClassifier(n_neighbors= 10)\nclassifier.fit(X_train, y_train)"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["# creating a prediction and producing a confusion matrix and classification report\n# we can see the results are very similar to the original decision tree model, just 1 false negative\n\ny_pred = classifier.predict(X_test)\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"markdown","source":["Lets refine down the features now using the same list we generated from the feature importance and try it with KNN modelling- see \"6. KNN- Final\""],"metadata":{}},{"cell_type":"markdown","source":[""],"metadata":{}}],"metadata":{"createdWith":"Filament"}}