{"nbformat":4,"nbformat_minor":0,"cells":[{"cell_type":"markdown","source":["# 4. ðŸŽ„ Dec Trees- Final\nExported from Filament on Thu, 17 Mar 2022 19:29:57 GMT\n\n---"],"metadata":{}},{"cell_type":"code","source":["import numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom time import time\n\nfrom sklearn import datasets\nfrom sklearn import metrics\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["df = pd.read_csv('decision_mushrooms.csv') # reading in the cleaned and encoded mushroom data"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"markdown","source":["### âœ… Functions:"],"metadata":{}},{"cell_type":"code","source":["#Our Accuracy, precision and recall custom fucntion\ndef apr(y_pred, y_real):\n    accuracy = metrics.accuracy_score(y_real, y_pred)\n    precision = metrics.precision_score(y_real, y_pred)\n    recall = metrics.recall_score(y_real, y_pred)\n    f1 = metrics.f1_score(y_real, y_pred)\n    \n    print(f\"Accuracy:{accuracy}\")\n    print(f\"Precision:{precision}\")\n    print(f\"Recall:{recall}\")\n    print(f\"F1:{f1}\")\n    return accuracy, precision, recall, f1\n\n\n#Confusion matrix function\ndef produce_confusion(positive_label, negative_label, cut_off, df, y_pred_name, y_real_name):\n    \n    #Set pred to 0 or 1 depending on whether it's higher than the cut_off point.\n    #We use this when we predict probabilites\n    if cut_off != 'binary':      \n        df['pred_binary'] = np.where(df[y_pred_name] > cut_off , 1, 0)\n    else: \n        df['pred_binary'] = df[y_pred_name]\n    \n    #Build the CM\n    cm = confusion_matrix(df[y_real_name], df['pred_binary'])  \n    \n    ax= plt.subplot()\n    sns.heatmap(cm, annot=True, ax=ax, fmt='g'); \n\n    # labels\n    ax.set_xlabel('Predicted labels');\n    ax.set_ylabel('Real labels'); \n    #Title\n    ax.set_title('Confusion Matrix'); \n    #Ticks\n    ax.xaxis.set_ticklabels([negative_label, positive_label])\n    ax.yaxis.set_ticklabels([negative_label, positive_label]);\n\n    print('Test accuracy = ', accuracy_score(df[y_real_name], df['pred_binary']))\n\n    return accuracy_score(df[y_real_name], df['pred_binary'])"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"markdown","source":["## âœ… Train/Test Split:"],"metadata":{}},{"cell_type":"markdown","source":["So here we have the refined feature columns to use for our final decision tree. Lets see how the results look, using just these 11 features!"],"metadata":{}},{"cell_type":"code","source":["feature_cols = ['gill_size', 'cap_surface_grooves', 'odor_almond', 'odor_anise', 'odor_none',\n    'stalk_root_bulbous', 'stalk_root_club', 'stalk_surface_below_ring_scaly',\n    'spore_print_color_green', 'population_clustered', 'habitat_woods']\ny = df['class']\nX = df[feature_cols]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=124, stratify=y)"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["# defining a random set of parameters again\n\ntreeclf = DecisionTreeClassifier(max_depth=3, random_state=1)\ntreeclf.fit(X, y)"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["plt.figure(figsize=(30,20))\ntree.plot_tree(treeclf, feature_names=feature_cols,  \n                class_names=['Poisonous','Edible'], filled=True)"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["# so even with these random params, the scores still look pretty good.\n# but lets see if we can improve on them further...\n\nprint(f'Score on training set: {treeclf.score(X_train, y_train)}')\nprint(f'Score on testing set: {treeclf.score(X_test, y_test)}')"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["# Using a grid search again to help us fine tune the parameters\n\n\ngrid = GridSearchCV(estimator = DecisionTreeClassifier(),\n                    param_grid = {'max_depth': [3, 5, 7, 10],\n                                  'min_samples_split': [5, 10, 15, 20],\n                                  'min_samples_leaf': [2, 3, 4, 5, 6, 7]},\n                    cv = 5,\n                    refit = True,\n                    verbose = 1,\n                    scoring = 'accuracy')"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["# wow- its 10 seconds faster to run using just this refined list of features, great!\n\nnow = time()\n\ngrid.fit(X_train, y_train)\n\nprint(f' Time in seconds: {time() - now}')"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["# here we can see the best params to use\n\ngrid.best_params_"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["# nice- the score still holds up when we refine\ngrid.best_score_"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"markdown","source":["# âœ… Final Decision Tree:"],"metadata":{}},{"cell_type":"code","source":["# you know the drill by now- lets run the tree with the best params\n\ndt = DecisionTreeClassifier(max_depth=5, min_samples_leaf=2, min_samples_split=5)"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["# fit the model\n\ndt.fit(X_train, y_train)"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["# nice- it looks really good, lets check the actual scores for confirmation\n\nfig = plt.figure(figsize=(30,20))\nthing = tree.plot_tree(dt, \n                   feature_names=feature_cols,  \n                   class_names=['Poisonous','Edible'],\n                   filled=True)"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["# and here we have it- these 11 refined features are still producing a really good score on train/test\n\nprint(f'Score on training set: {dt.score(X_train, y_train)}')\nprint(f'Score on testing set: {dt.score(X_test, y_test)}')"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["# here we've created a probability of our predictions being right for the train data\n\ntrain_results = X_train.copy()\ntrain_results['y_pred'] = dt.predict(X_train)\ntrain_results['y_real'] = y_train\ntrain_results['y_prob'] = dt.predict_proba(X_train)[:,1]\n\ntrain_results"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["# and we've done the same for the test here\n\ntest_results = X_test.copy()\ntest_results['y_pred'] = dt.predict(X_test)\ntest_results['y_real'] = y_test\ntest_results['y_prob'] = dt.predict_proba(X_test)[:,1]\n\ntest_results"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"markdown","source":["## âœ… Confusion Matrix:"],"metadata":{}},{"cell_type":"code","source":["# so here we can see that the results are pretty good- but beware, there are still 2 false negatives \n\n\"\"\" Confusion Matrix for Train Data \"\"\"\nproduce_confusion('Poisonous', 'Edible', 0.5, train_results, 'y_pred', 'y_real')"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"code","source":["# similarly for the test data- we still have two false negatives!\n\n\"\"\" Confusion Matrix for Test Data \"\"\"\nproduce_confusion('Poisonous','Edible', 0.5, test_results, 'y_pred', 'y_real')"],"outputs":[],"metadata":{},"execution_count":null},{"cell_type":"markdown","source":[""],"metadata":{}},{"cell_type":"markdown","source":[""],"metadata":{}}],"metadata":{"createdWith":"Filament"}}